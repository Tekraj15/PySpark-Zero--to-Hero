{"cells":[{"cell_type":"markdown","metadata":{"editable":true,"trusted":true},"source":["# AWS Glue Studio Notebook\n","##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n"]},{"cell_type":"markdown","metadata":{"editable":true,"trusted":true},"source":["#### Optional: Run this cell to see available notebook commands (\"magics\").\n"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":true,"trusted":true,"vscode":{"languageId":"python_glue_session"}},"outputs":[],"source":["%help"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true,"vscode":{"languageId":"python_glue_session"}},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\u001b[0m\n","Defaulting to user installation because normal site-packages is not writeable\n","Collecting matplotlib\n","  Downloading matplotlib-2.2.5-cp27-cp27mu-manylinux1_x86_64.whl (12.8 MB)\n","\u001b[K     |████████████████████████████████| 12.8 MB 38.2 MB/s eta 0:00:01\n","\u001b[?25hCollecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1\n","  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 7.4 MB/s  eta 0:00:01\n","\u001b[?25hCollecting backports.functools-lru-cache\n","  Downloading backports.functools_lru_cache-1.6.4-py2.py3-none-any.whl (5.9 kB)\n","Collecting pytz\n","  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n","Collecting six>=1.10\n","  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n","Collecting python-dateutil>=2.1\n","  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n","Collecting cycler>=0.10\n","  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n","Collecting numpy>=1.7.1\n","  Downloading numpy-1.16.6-cp27-cp27mu-manylinux1_x86_64.whl (17.0 MB)\n","\u001b[K     |████████████████████████████████| 17.0 MB 77.8 MB/s eta 0:00:01\n","\u001b[?25hCollecting kiwisolver>=1.0.1\n","  Downloading kiwisolver-1.1.0-cp27-cp27mu-manylinux1_x86_64.whl (93 kB)\n","\u001b[K     |████████████████████████████████| 93 kB 3.2 MB/s  eta 0:00:01\n","\u001b[?25hCollecting subprocess32\n","  Downloading subprocess32-3.5.4-cp27-cp27mu-manylinux2014_x86_64.whl (69 kB)\n","\u001b[K     |████████████████████████████████| 69 kB 12.7 MB/s eta 0:00:01\n","\u001b[?25hRequirement already satisfied: setuptools in /usr/lib/python2.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (41.2.0)\n","Installing collected packages: pyparsing, backports.functools-lru-cache, pytz, six, python-dateutil, cycler, numpy, kiwisolver, subprocess32, matplotlib\n","\u001b[33m  WARNING: The scripts f2py, f2py2 and f2py2.7 are installed in '/home/jupyter-user/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n","Successfully installed backports.functools-lru-cache-1.6.4 cycler-0.10.0 kiwisolver-1.1.0 matplotlib-2.2.5 numpy-1.16.6 pyparsing-2.4.7 python-dateutil-2.8.2 pytz-2023.3 six-1.16.0 subprocess32-3.5.4\n"]}],"source":["!pip install matplotlib"]},{"cell_type":"markdown","metadata":{"editable":true,"trusted":true},"source":["####  Run this cell to set up and start your interactive session.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"editable":true,"trusted":true,"vscode":{"languageId":"python_glue_session"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Welcome to the Glue Interactive Sessions Kernel\n","For more information on available magic commands, please type %help in any new cell.\n","\n","Please view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\n","Installed kernel version: 0.37.3 \n","Current idle_timeout is 2800 minutes.\n","idle_timeout has been set to 2880 minutes.\n","Setting Glue version to: 3.0\n","Previous worker type: G.1X\n","Setting new worker type to: G.1X\n","Previous number of workers: 5\n","Setting new number of workers to: 4\n","Authenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::321914074467:role/Tekraj-AWS-Roles\n","Trying to create a Glue session for the kernel.\n","Worker Type: G.1X\n","Number of Workers: 4\n","Session ID: 06936e71-d8dd-4e9e-a745-58b15240d5d6\n","Job Type: glueetl\n","Applying the following default arguments:\n","--glue_kernel_version 0.37.3\n","--enable-glue-datacatalog true\n","Waiting for session 06936e71-d8dd-4e9e-a745-58b15240d5d6 to get into ready status...\n","Session 06936e71-d8dd-4e9e-a745-58b15240d5d6 has been created.\n","GlueArgumentError: the following arguments are required: --JOB_NAME\n"]}],"source":["%idle_timeout 2880\n","%glue_version 3.0\n","%worker_type G.1X\n","%number_of_workers 4\n","\n","import sys\n","from awsglue.transforms import *\n","from awsglue.utils import getResolvedOptions\n","from pyspark.context import SparkContext\n","from awsglue.context import GlueContext\n","from awsglue.job import Job\n","from pyspark.sql.functions import to_timestamp, from_utc_timestamp\n","from pyspark.sql.functions import *\n","#from pyspark.sql.functions import avg, mean, min, max, last, stddev, window, round, cast\n","  \n","args = getResolvedOptions(sys.argv, [\"JOB_NAME\"])\n","sc = SparkContext()\n","glueContext = GlueContext(sc)\n","spark = glueContext.spark_session\n","job = Job(glueContext)\n","job.init(args[\"JOB_NAME\"], args)"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true,"vscode":{"languageId":"python_glue_session"}},"outputs":[{"name":"stdout","output_type":"stream","text":["s3_path is :  s3://tekraj-test2/sparkCognition_Data_Analysis/Datasets/DataSet/Device1_2020_07_01_00_00_02.969_2020_07_31_23_59_58.110.csv\n","+--------------------+---------+-----+-------+\n","|           TimeStamp| variable|value| device|\n","+--------------------+---------+-----+-------+\n","|2020-07-01 00:00:...|MMXN1_Amp| null|Device1|\n","|2020-07-01 00:00:...|MMXN1_Amp| null|Device1|\n","|2020-07-01 00:00:...|MMXN1_Amp|846.0|Device1|\n","|2020-07-01 00:00:...|MMXN1_Amp| null|Device1|\n","|2020-07-01 00:00:...|MMXN1_Amp|769.0|Device1|\n","|2020-07-01 00:00:...|MMXN1_Amp|732.0|Device1|\n","|2020-07-01 00:00:...|MMXN1_Amp| null|Device1|\n","|2020-07-01 00:00:...|MMXN1_Amp| null|Device1|\n","|2020-07-01 00:00:...|MMXN1_Amp|644.0|Device1|\n","|2020-07-01 00:00:...|MMXN1_Amp| null|Device1|\n","+--------------------+---------+-----+-------+\n","only showing top 10 rows\n","\n","root\n"," |-- TimeStamp: timestamp (nullable = true)\n"," |-- variable: string (nullable = true)\n"," |-- value: double (nullable = true)\n"," |-- device: string (nullable = true)\n"]}],"source":["s3_bucket = 'tekraj-test2'\n","preffix = 'sparkCognition_Data_Analysis/Datasets/DataSet/Device1_2020_07_01_00_00_02.969_2020_07_31_23_59_58.110.csv'\n","s3_path = 's3://'+s3_bucket+'/'+preffix\n","\n","print('s3_path is : ', s3_path)\n","\n","df = spark.read.csv(s3_path, header= True, inferSchema= True)\n","\n","df = df.withColumn(\"TimeStamp\", to_timestamp(\"TimeStamp\"))\n","\n","#df = df.withColumn(\"TimeStamp\",to_timestamp(\"TimeStamp\",'yyyy-MM-dd HH:mm:ss'))\n","df.show(10)\n","df.printSchema()\n"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true,"vscode":{"languageId":"python_glue_session"}},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"]}],"source":["no_of_rows = df.count()"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true,"vscode":{"languageId":"python_glue_session"}},"outputs":[{"name":"stdout","output_type":"stream","text":["34164004\n"]}],"source":["# Counting the number of Null values in 'value' columns \n","null_counts = df.filter(col(\"value\").isNull()).count()\n","print(null_counts)"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true,"vscode":{"languageId":"python_glue_session"}},"outputs":[{"name":"stdout","output_type":"stream","text":["88.20707851136501\n"]}],"source":["percent_of_nulls = null_counts*100.0/(no_of_rows)\n","print(percent_of_nulls)"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true,"vscode":{"languageId":"python_glue_session"}},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+---------+-----+-------+\n","|           TimeStamp| variable|value| device|\n","+--------------------+---------+-----+-------+\n","|2020-07-01 00:00:...|MMXN1_Amp|232.0|Device1|\n","|2020-07-01 00:00:...|MMXN1_Amp|232.0|Device1|\n","|2020-07-01 00:00:...|MMXN1_Amp|846.0|Device1|\n","|2020-07-01 00:00:...|MMXN1_Amp|232.0|Device1|\n","|2020-07-01 00:00:...|MMXN1_Amp|769.0|Device1|\n","|2020-07-01 00:00:...|MMXN1_Amp|732.0|Device1|\n","|2020-07-01 00:00:...|MMXN1_Amp|232.0|Device1|\n","|2020-07-01 00:00:...|MMXN1_Amp|232.0|Device1|\n","|2020-07-01 00:00:...|MMXN1_Amp|644.0|Device1|\n","|2020-07-01 00:00:...|MMXN1_Amp|232.0|Device1|\n","|2020-07-01 00:00:...|MMXN1_Amp|605.0|Device1|\n","|2020-07-01 00:00:...|MMXN1_Amp|232.0|Device1|\n","|2020-07-01 00:00:...|MMXN1_Amp|232.0|Device1|\n","|2020-07-01 00:00:...|MMXN1_Amp|232.0|Device1|\n","|2020-07-01 00:00:...|MMXN1_Amp|232.0|Device1|\n","|2020-07-01 00:00:...|MMXN1_Amp|609.0|Device1|\n","|2020-07-01 00:00:...|MMXN1_Amp|232.0|Device1|\n","|2020-07-01 00:00:...|MMXN1_Amp|232.0|Device1|\n","|2020-07-01 00:00:...|MMXN1_Amp|645.0|Device1|\n","|2020-07-01 00:00:...|MMXN1_Amp|662.0|Device1|\n","+--------------------+---------+-----+-------+\n","only showing top 20 rows\n"]}],"source":["# Handle NaN values by replacing them with the Median of each variable as the percenatge of Missing values is very High\n","\n","median_value = df.selectExpr(\"percentile_approx(value, 0.5)\").collect()[0][0]\n","\n","df_new = df.withColumn(\"value\", when(col(\"value\").isNull(), median_value).otherwise(col(\"value\")))\n","\n","df_new.show()"]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true,"vscode":{"languageId":"python_glue_session"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Error: Interpreter died:\n","\n"]}],"source":["import matplotlib.pyplot as plt\n","\n","# Plot the distribution before handling NaN values\n","variable_dist_before = df.select(\"variable\", \"value\").toPandas()\n","plt.scatter(variable_dist_before[\"variable\"], variable_dist_before[\"value\"], alpha=0.5)\n","plt.xlabel(\"Variable\")\n","plt.ylabel(\"Value\")\n","plt.title(\"Distribution of Variables Before Handling NaN\")\n","plt.show()\n","\n","# # Save the scatter plot image to an in-memory buffer, as AWS Glue Notebook is\n","# buffer = io.BytesIO()\n","# plt.savefig(buffer, format='png')\n","# buffer.seek(0)\n","\n","# # Upload the image buffer to S3\n","# s3 = boto3.client('s3')\n","# s3.upload_fileobj(buffer, s3_bucket, 'distribution_before.png')"]},{"cell_type":"code","execution_count":17,"metadata":{"trusted":true,"vscode":{"languageId":"python_glue_session"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Error: Interpreter died:\n","\n"]}],"source":["# Plot the distribution after handling NaN values\n","variable_dist_after = df_new.select(\"variable\", \"value\").toPandas()\n","plt.scatter(variable_dist_after[\"variable\"], variable_dist_after[\"value\"], alpha=0.5)\n","plt.xlabel(\"Variable\")\n","plt.ylabel(\"Value\")\n","plt.title(\"Distribution of Variables After Handling NaN\")\n","plt.show()"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true,"vscode":{"languageId":"python_glue_session"}},"outputs":[{"name":"stdout","output_type":"stream","text":["30\n"]}],"source":["#Counting distinct values in 'variable' column\n","\n","distinct_variable = df_new.select(\"variable\").distinct()\n","distinct_variable.count()"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python_glue_session"}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true,"vscode":{"languageId":"python_glue_session"}},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+-------+---------+------+-----+---------------+------+------+\n","|           TimeStamp| device| variable|   avg|  min|            max|  last|stddev|\n","+--------------------+-------+---------+------+-----+---------------+------+------+\n","|{2020-07-01 00:00...|Device1|MMXN1_Amp|701.32|232.0|         1936.0|1728.0|551.25|\n","|{2020-07-01 00:40...|Device1|MMXN1_Amp|480.09|232.0|         1155.0| 232.0|297.68|\n","|{2020-07-01 03:20...|Device1|MMXN1_Amp| 363.5|232.0|          616.0| 332.0|129.18|\n","|{2020-07-01 05:40...|Device1|MMXN1_Amp|346.13|232.0|          628.0| 232.0|129.29|\n","|{2020-07-01 06:50...|Device1|MMXN1_Amp|406.57|232.0|992.42163085938| 232.0|197.48|\n","|{2020-07-01 07:20...|Device1|MMXN1_Amp| 454.4|232.0|1182.5799560547| 232.0|252.49|\n","|{2020-07-01 08:10...|Device1|MMXN1_Amp|608.42|232.0|         1251.0| 924.0|378.73|\n","|{2020-07-01 09:40...|Device1|MMXN1_Amp| 689.5|232.0|         1551.0|1093.0|470.17|\n","|{2020-07-01 11:10...|Device1|MMXN1_Amp|558.14|232.0|         1344.0| 232.0| 358.8|\n","|{2020-07-01 14:00...|Device1|MMXN1_Amp| 504.9|232.0|          964.0| 850.0|290.16|\n","|{2020-07-01 17:50...|Device1|MMXN1_Amp|224.58| 74.0|          420.0| 232.0| 58.41|\n","|{2020-07-01 18:40...|Device1|MMXN1_Amp|225.36|148.0|          347.0| 162.0| 30.53|\n","|{2020-07-01 19:10...|Device1|MMXN1_Amp|162.74| 29.0|          232.0| 232.0| 75.74|\n","|{2020-07-01 19:40...|Device1|MMXN1_Amp|165.77| 26.0|          232.0| 232.0|  79.9|\n","|{2020-07-01 21:30...|Device1|MMXN1_Amp|187.45| 65.0|          248.0| 232.0| 53.24|\n","|{2020-07-01 22:40...|Device1|MMXN1_Amp|226.32| 33.0|          399.0| 176.0|  63.2|\n","|{2020-07-02 00:20...|Device1|MMXN1_Amp|571.05|232.0|         1336.0| 906.0|378.26|\n","|{2020-07-02 00:30...|Device1|MMXN1_Amp|487.76|232.0|         1099.0| 232.0| 277.9|\n","|{2020-07-02 00:40...|Device1|MMXN1_Amp| 477.1|232.0|          968.0| 232.0|255.06|\n","|{2020-07-02 05:50...|Device1|MMXN1_Amp|938.09|232.0|         1990.0|1471.0|707.63|\n","+--------------------+-------+---------+------+-----+---------------+------+------+\n","only showing top 20 rows\n"]}],"source":["# Perform 10-minute aggregations for each column\n","\n","window_duration = \"10 minutes\"\n","\n","aggregated_df = df_new.groupBy(window(\"TimeStamp\", window_duration).alias(\"TimeStamp\"), \"device\", \"variable\").agg(\n","    round(avg(\"value\"), 2).alias(\"avg\"),\n","    round(min(\"value\"), 2).alias(\"min\"),\n","    max(\"value\").alias(\"max\"),\n","    last(\"value\").alias(\"last\"),\n","    round(stddev(\"value\"), 2).alias(\"stddev\")\n",")\n","\n","\n","# for column_name in aggregated_df.columns:\n","#     if column_name not in [\"window\", \"device\"]:\n","#         variable = column_name.split(\"_\")[0]\n","#         new_column_name = f\"{variable}_{column_name}\"\n","#         aggregated_df = aggregated_df.withColumnRenamed(column_name, new_column_name)\n","\n","\n","# # Show the resulting aggregated dataframe\n","# aggregated_df.show(truncate=False)\n","\n","\n","aggregated_df.sort(\"variable\", \"device\", \"window\")\n","aggregated_df.show()"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true,"vscode":{"languageId":"python_glue_session"}},"outputs":[{"name":"stdout","output_type":"stream","text":["AnalysisException: grouping expressions sequence is empty, and '`TimeStamp`' is not an aggregate function. Wrap '(avg(`value`) AS `WROT1_RotSpd_avg`)' in windowing function(s) or wrap '`TimeStamp`' in first() (or first_value) if you don't care which value you get.;\n","Aggregate [TimeStamp#48, variable#41, value#90, device#43, avg(value#90) AS WROT1_RotSpd_avg#379]\n","+- Deduplicate [TimeStamp#48, variable#41, value#90, device#43]\n","   +- Filter (variable#41 = WROT1_RotSpd)\n","      +- Project [TimeStamp#48, variable#41, CASE WHEN isnull(value#42) THEN 232.0 ELSE value#42 END AS value#90, device#43]\n","         +- Project [to_timestamp('TimeStamp, None) AS TimeStamp#48, variable#41, value#42, device#43]\n","            +- Relation[TimeStamp#40,variable#41,value#42,device#43] csv\n","\n"]}],"source":["columns = ['TimeStamp', 'device']\n","\n","for i in df_new.select(\"variable\").distinct().collect():\n","    variable = i.variable\n","    \n","    df2 = df_new.filter(col(\"variable\") == variable).distinct()\n","    \n","    # Adding new column with prefix 'variable_' for avg, min, max, last, and stddev\n","    df2 = df2.withColumn(variable + '_' + 'avg', avg(\"value\"))\n","    df2 = df2.withColumn(variable + '_' + 'min', min(\"value\"))\n","    df2 = df2.withColumn(variable + '_' + 'max', max(\"value\"))\n","    df2 = df2.withColumn(variable + '_' + 'last', last(\"value\"))\n","    df2 = df2.withColumn(variable + '_' + 'stddev', stddev(\"value\"))\n","    \n","    columns.extend([variable + '_avg', variable + '_min', variable + '_max', variable + '_last', variable + '_stddev'])\n","    \n","\n","# Select the desired columns and sort the DataFrame\n","df3 = df_new.select(columns).distinct().sort(\"variable\", \"device\", \"TimeStamp\")\n","\n","df3.show()\n"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true,"vscode":{"languageId":"python_glue_session"}},"outputs":[{"name":"stdout","output_type":"stream","text":["AnalysisException: grouping expressions sequence is empty, and '`TimeStamp`' is not an aggregate function. Wrap '(avg(`value`) AS `WNAC1_WdSpd2_avg`)' in windowing function(s) or wrap '`TimeStamp`' in first() (or first_value) if you don't care which value you get.;\n","Aggregate [TimeStamp#24, variable#17, value#57, device#19, avg(value#57) AS WNAC1_WdSpd2_avg#207]\n","+- Deduplicate [TimeStamp#24, variable#17, value#57, device#19]\n","   +- Filter (variable#17 = WNAC1_WdSpd2)\n","      +- Project [TimeStamp#24, variable#17, CASE WHEN isnull(value#18) THEN 232.0 ELSE value#18 END AS value#57, device#19]\n","         +- Project [to_timestamp('TimeStamp, None) AS TimeStamp#24, variable#17, value#18, device#19]\n","            +- Relation[TimeStamp#16,variable#17,value#18,device#19] csv\n","\n"]}],"source":["columns = ['TimeStamp', 'device']\n","\n","for i in df_new.select(\"variable\").distinct().collect():\n","    variable = i.variable\n","    \n","    df2 = df_new.filter(col(\"variable\") == variable).distinct()\n","    \n","    # Adding new column with prefix 'variable_' for avg, min, max, last, and stddev\n","    df2 = df2.withColumn(variable + '_' + 'avg', avg(\"value\"))\n","    df2 = df2.withColumn(variable + '_' + 'min', min(\"value\"))\n","    df2 = df2.withColumn(variable + '_' + 'max', max(\"value\"))\n","    df2 = df2.withColumn(variable + '_' + 'last', last(\"value\"))\n","    df2 = df2.withColumn(variable + '_' + 'stddev', stddev(\"value\"))\n","    \n","    columns.extend([variable + '_avg', variable + '_min', variable + '_max', variable + '_last', variable + '_stddev'])\n","    \n","\n","# Select the desired columns and sort the DataFrame\n","df3 = df_new.groupBy(\"TimeStamp\", \"device\", *columns).agg().sort(\"variable\", \"device\", \"TimeStamp\")\n","\n","df3.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python_glue_session"}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"vscode":{"languageId":"python_glue_session"}},"outputs":[],"source":["grouped_df = df_new.groupBy(\"TimeStamp\", \"device\", \"variable\")\n","\n","# Generate a dynamic list of aggregation expressions for each distinct value in 'variable'\n","aggregation_exprs = []\n","distinct_variables = df.select(\"variable\").distinct().rdd.flatMap(lambda x: x).collect()\n","for variable in distinct_variables:\n","    aggregation_exprs.extend([\n","        round(avg(df[\"value\"]).alias(variable + \"_avg\"), 2),\n","        round(min(df[\"value\"]).alias(variable + \"_min\"), 2),\n","        max(df[\"value\"]).alias(variable + \"_max\")\n","    ])\n","\n","# Perform the aggregations using the generated expressions\n","aggregated_df = grouped_df.agg(*aggregation_exprs)\n","\n","# Print the schema of the aggregated DataFrame\n","aggregated_df.printSchema()\n","\n","# Show the contents of the aggregated DataFrame\n","aggregated_df.show()\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python_glue_session"}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python_glue_session"}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python_glue_session"}},"outputs":[],"source":["# General function for getting n-minute (window duration) aggregate \n","\n","def get_aggregates(df, window_duration, columns):\n","    return df.groupBy(window(\"TimeStamp\", window_duration)).agg(\n","        avg(*columns).alias(\"avg\"),\n","        min(*columns).alias(\"min\"),\n","        max(*columns).alias(\"max\"),\n","        last(*columns).alias(\"last\"),\n","        stddev(*columns).alias(\"stddev\")\n","    )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"python_glue_session"}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"editable":true,"jp-MarkdownHeadingCollapsed":true,"tags":[],"trusted":true},"source":["#### Example: Create a DynamicFrame from a table in the AWS Glue Data Catalog and display its schema\n"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":true,"trusted":true,"vscode":{"languageId":"python_glue_session"}},"outputs":[],"source":["dyf = glueContext.create_dynamic_frame.from_catalog(database='database_name', table_name='table_name')\n","dyf.printSchema()"]},{"cell_type":"markdown","metadata":{"editable":true,"trusted":true},"source":["#### Example: Convert the DynamicFrame to a Spark DataFrame and display a sample of the data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":true,"trusted":true,"vscode":{"languageId":"python_glue_session"}},"outputs":[],"source":["df = dyf.toDF()\n","df.show()"]},{"cell_type":"markdown","metadata":{"editable":true,"trusted":true},"source":["#### Example: Write the data in the DynamicFrame to a location in Amazon S3 and a table for it in the AWS Glue Data Catalog\n"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":true,"trusted":true,"vscode":{"languageId":"python_glue_session"}},"outputs":[],"source":["s3output = glueContext.getSink(\n","  path=\"s3://bucket_name/folder_name\",\n","  connection_type=\"s3\",\n","  updateBehavior=\"UPDATE_IN_DATABASE\",\n","  partitionKeys=[],\n","  compression=\"snappy\",\n","  enableUpdateCatalog=True,\n","  transformation_ctx=\"s3output\",\n",")\n","s3output.setCatalogInfo(\n","  catalogDatabase=\"demo\", catalogTableName=\"populations\"\n",")\n","s3output.setFormat(\"glueparquet\")\n","s3output.writeFrame(DyF)"]}],"metadata":{"kernelspec":{"display_name":"Glue PySpark","language":"python","name":"glue_pyspark"},"language_info":{"codemirror_mode":{"name":"python","version":3},"file_extension":".py","mimetype":"text/x-python","name":"Python_Glue_Session","pygments_lexer":"python3"},"toc-autonumbering":false},"nbformat":4,"nbformat_minor":4}
